# Data modelling with postgres: Sparkify database
In this project, you'll apply what you've learned on data modeling with Postgres and build an ETL pipeline using Python. To complete the project, you will need to define fact and dimension tables for a star schema for a particular analytic focus, and write an ETL pipeline that transfers data from files in two local directories into these tables in Postgres using Python and SQL.

## Purpose of the database
Sparkify is a startup in the music streaming business. To optimize their products they need to know insights about user/song/streaming activity: what songs and artists are most listened to. At the moment they don't have an easy way to get these insights.
The data sparkify has at the moment is stored in to dataset (JSON format)
1. Song-dataset: The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains     metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are file paths to two files in this dataset.
2. Log-dataset: The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

With this project we want to create an SQL database that is structured in a certain way that makes it easy to find insights in the underlying datasets.   

## How to run python scrypts
    first step: run following command '! python create_tables.py'
    second step: run following command '! python etl.py'
    third step: check in test.ipynb if tables are populated and all queries runned without errors or warnings
## explanation of the files
    1. create_tables.py: drops and creates your tables. You run this file to reset your tables before each time you run your ETL scripts
    2. etl.ipynb: reads and processes a single file from song_data and log_data and loads the data into your tables. This notebook     contains detailed instructions on the ETL process for each of the tables.
    3. etl.py: reads and processes files from song_data and log_data and loads them into your tables. You can fill this out based on your work in the ETL notebook.
    4. README.md: provides discussion on your project.
    5. sql_queries.py: contains all your sql queries, and is imported into the last three files above.
    6. test.ipynb:displays the first few rows of each table to let you check your database.
## Database schema
The database we created is created in a star schema and consist of 5 tables with each their own primary key:
1. Fact Table 
    **songplays**
    Records in log data associated with song plays i.e. records with page NextSong
    Variables: songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent
2. Dimension Tables
    **users**
    users in the app
    Variables: user_id, first_name, last_name, gender, level
    
    **songs**
    songs in music database
    Variables:song_id, title, artist_id, year, duration
    
    **artists**
    artists in music database
    Variables:artist_id, name, location, latitude, longitude
    
    **time**
    timestamps of records in songplays broken down into specific units
    Variables:start_time, hour, day, week, month, year, weekday